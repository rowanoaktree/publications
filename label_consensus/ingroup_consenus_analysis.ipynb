{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving consensus annotations and analyzing agreement within observer groups\n",
    "\n",
    "Analytic code supporting \"Observer variability in manual-visual interpretation of aerial imagery of wildlife, with implications for deep learning\" - Converse et al. submitted Feb 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import sklearn.metrics\n",
    "from shapely.geometry import Polygon,Point\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely\n",
    "import cv2 as cv\n",
    "import os\n",
    "import gc\n",
    "\n",
    "#Data loading\n",
    "path = \"path/to/labels.csv\"\n",
    "with open(path) as f:\n",
    "    raw = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bounding box area; area by class:\n",
    "\n",
    "#Calculate area of bounding boxes\n",
    "def calc_area(row):\n",
    "    bbox = row['bbox']\n",
    "    xmin, ymin, w, h = bbox\n",
    "    return w * h\n",
    "\n",
    "raw['area'] = raw.apply(calc_area, axis=1)\n",
    "\n",
    "#Determine average area of bounding box per class\n",
    "raw.groupby(\"class_id\")[\"area\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive consensus bounding boxes with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import patches, text, patheffects\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import scipy.stats\n",
    "\n",
    "#Define parameters for DBSCAN\n",
    "\n",
    "def group_DBSCAN(df):\n",
    "    x = df[[\"c_x\", \"c_y\"]].to_numpy()\n",
    "    cluster = DBSCAN(eps=15, min_samples=5).fit(x)\n",
    "    labels = cluster.labels_\n",
    "    df[\"cluster_id\"] = labels\n",
    "    return labels\n",
    "\n",
    "#Derive bounding box centers as inputs for DBSCAN\n",
    "bboxes = raw[\"bbox\"]\n",
    "c_x = []\n",
    "c_y = []\n",
    "x = []\n",
    "y = []\n",
    "w = []\n",
    "h = []\n",
    "centers = []\n",
    "for coord in bboxes:\n",
    "    center = (coord[0]+(coord[2]/2), coord[1]+(coord[3]/2))\n",
    "    c_x.append(center[0])\n",
    "    c_y.append(center[1])\n",
    "    x.append(coord[0])\n",
    "    y.append(coord[1])\n",
    "    w.append(coord[2])\n",
    "    h.append(coord[3])\n",
    "    centers.append(center)\n",
    "#Make these centers into a coordinate format\n",
    "coords = []\n",
    "for row in centers:\n",
    "    coord = list(row)\n",
    "    coords.append(coord)\n",
    "#Append new columns to dataframe\n",
    "raw[\"c_x\"] = c_x\n",
    "raw[\"c_y\"] = c_y\n",
    "raw['x'] = x\n",
    "raw['y'] = y\n",
    "raw['w'] = w\n",
    "raw['h'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify clusters of labels with DBSCAN; append cluster ID to individual labels\n",
    "\n",
    "clusters = raw.groupby(\"filename\").apply(lambda x: group_DBSCAN(x))\n",
    "clusters = clusters.reset_index()\n",
    "clusters.rename(columns = {0:'cluster_id'}, inplace=True)\n",
    "long = clusters.explode(\"cluster_id\")\n",
    "long.reset_index()\n",
    "filesort = raw.sort_values([\"filename\", \"annotation_id\"])\n",
    "filesort.reset_index()\n",
    "test = filesort.reset_index().merge(long.reset_index(), left_index=True, right_index=True, how='left')\n",
    "test = test.drop(columns=['filename_y','index_y'])\n",
    "test = test.rename(columns={'filename_x':'filename', 'index_x': 'index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive consensus labels\n",
    "\n",
    "#Dictionary of category values\n",
    "categories = {1: \"Crane\", 2: 'Goose', 3:'Duck', 4: 'Other Bird'}\n",
    "\n",
    "#Derive refined products: median coordinates, and plurality vote class ID\n",
    "refined = test.groupby(['filename', 'cluster_id']).agg({'x':'median', \n",
    "                         'y':'median', \n",
    "                         'w':'median', \n",
    "                         'h': 'median',\n",
    "                         'category_id': pd.Series.mode}).reset_index()\n",
    "#Make median bounding box into its own column in list form\n",
    "refined['bbox']= refined[['x','y','w','h']].values.tolist()\n",
    "#Remove rows where no agreement was reached on class ID\n",
    "rowcount = len(refined)\n",
    "refined_drop = refined[refined['category_id'].isin([1,2,3,4])]\n",
    "agreement = len(refined_drop)\n",
    "removed = rowcount - agreement\n",
    "#Remove noise points\n",
    "refined_id = refined_drop[refined_drop['cluster_id'] != -1]\n",
    "final = len(refined_id)\n",
    "noise = rowcount - final\n",
    "print(\"Removed rows: %s due to no class ID agreement; %s noise points\" %(removed, noise))\n",
    "refined_id['category'] = refined_id[\"category_id\"].map(categories)\n",
    "refined_id = refined_id.drop(columns=['x','y','w','h'])\n",
    "\n",
    "#Create analysis dataframe that has raw annotations with the corresponding consensus annotation\n",
    "df = test.merge(refined_id, left_on=['filename','cluster_id'], right_on = ['filename','cluster_id'], how='left')\n",
    "df = df.drop(columns=['c_x','c_y','area','x','y','w','h'])\n",
    "df = df.rename(columns={'bbox_x': 'bbox_orig', 'category_id_x': 'cat_id_orig', \"category_x\": \"cat_orig\", 'bbox_y': 'bbox_refined', 'category_id_y': 'cat_id_refined', \"category_y\": \"cat_refined\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting number of dropped annotations that were not matched with a cluster\n",
    "missing = df[df[\"cluster_id\"] == -1]\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID and Locational Agreement between raw and consensus labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate agreement between each raw label ID and the corresponding consensus label ID\n",
    "df['agree'] = 0\n",
    "df.loc[df['cat_orig'] == df[\"cat_refined\"], 'agree'] = 1\n",
    "\n",
    "#Calculate statistics on agreement overall and per class\n",
    "grouped_data = df.groupby(\"cat_refined\")[\"agree\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "print(df['agree'].mean())\n",
    "print(df['agree'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating IOU between each raw bounding box and the corresponding consensus box\n",
    "from shapely.geometry import box\n",
    "\n",
    "def eval_bbox(row, col_name):\n",
    "    bbox_str = row[col_name]\n",
    "    if pd.notnull(bbox_str):\n",
    "        bbox = np.array(ast.literal_eval(bbox_str))\n",
    "        bbox = bbox.astype(float)\n",
    "    else:\n",
    "        bbox = np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "    return bbox\n",
    "\n",
    "# Define a function to calculate the IOU only if both bounding boxes are non-null\n",
    "def calculate_iou(row):\n",
    "    bbox_orig = eval_bbox(row, 'bbox_orig')\n",
    "    bbox_ref = eval_bbox(row, 'bbox_refined')\n",
    "    if np.isnan(bbox_orig[0]) or np.isnan(bbox_orig[1]) or np.isnan(bbox_orig[2]) or np.isnan(bbox_orig[3]) or \\\n",
    "        np.isnan(bbox_ref[0]) or np.isnan(bbox_ref[1]) or np.isnan(bbox_ref[2]) or np.isnan(bbox_ref[3]):\n",
    "        iou = None\n",
    "    else:\n",
    "        bbox_orig = box(bbox_orig[0], bbox_orig[1], bbox_orig[0] + bbox_orig[2], bbox_orig[1] + bbox_orig[3])\n",
    "        bbox_ref = box(bbox_ref[0], bbox_ref[1], bbox_ref[0] + bbox_ref[2], bbox_ref[1] + bbox_ref[3])\n",
    "        iou = bbox_orig.intersection(bbox_ref).area / bbox_orig.union(bbox_ref).area\n",
    "    return iou\n",
    "\n",
    "# Apply the function to each row of the DataFrame and save the results in a new column\n",
    "df['IOU'] = df.apply(calculate_iou, axis=1)\n",
    "\n",
    "#IOU statistics overall and per class\n",
    "df[\"IOU\"].mean()\n",
    "df.groupby(\"cat_refined\")[\"IOU\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATING PIELOU'S INDEX\n",
    "\n",
    "# Group the dataframe by image, then by cluster\n",
    "grouped = df.groupby(['filename', 'cluster_id'])\n",
    "\n",
    "# Create empty lists to store the results\n",
    "cluster_id_list = []\n",
    "filename_list = []\n",
    "consensus_class_id_list = []\n",
    "class_count_list = []\n",
    "bbox_list = []\n",
    "pielou_index_list = []\n",
    "\n",
    "# Loop through each group and calculate Pielou's evenness index\n",
    "for name, group in grouped:\n",
    "    # Get the cluster ID, filename, and consensus class ID for this group\n",
    "    cluster_id = name[1]\n",
    "    filename = name[0]\n",
    "    #ADJUST LINE BELOW FOR SPP VS SUPERCLASS\n",
    "    consensus_bbox = group['bbox_refined'].iloc[0] \n",
    "    consensus_class_id = group['cat_refined'].iloc[0]  # Assumes all consensus IDs in the group are the same\n",
    "    \n",
    "    # Count the number of annotations in the group\n",
    "    num_annotations = len(group)\n",
    "    \n",
    "    # Count the number of annotations for each original class ID (ADJUST HERE FOR SPP VS SUPERCLASS)\n",
    "    class_counts = group.groupby('orig_superclass').size().values\n",
    "\n",
    "    #Score complete agreement as zero; otherwise, calculate Pielou:\n",
    "    if len(class_counts) == 1:\n",
    "        evenness_index = 0\n",
    "    else:\n",
    "        # Calculate the relative abundance of each original class ID\n",
    "        relative_abundance = class_counts / num_annotations\n",
    "    \n",
    "        # Calculate the evenness index using Pielou's formula\n",
    "        evenness_index = -np.sum(relative_abundance * np.log(relative_abundance)) / np.log(len(relative_abundance))\n",
    "    \n",
    "    # Append the results to the lists\n",
    "    cluster_id_list.append(cluster_id)\n",
    "    filename_list.append(filename)\n",
    "    consensus_class_id_list.append(consensus_class_id)\n",
    "    #agreement = class_count_list.append(class_counts)\n",
    "    bbox_list.append(consensus_bbox)\n",
    "    pielou_index_list.append(evenness_index)\n",
    "\n",
    "# Create a new dataframe with the results\n",
    "pielou = pd.DataFrame({\n",
    "    'cluster_id': cluster_id_list,\n",
    "    'filename': filename_list,\n",
    "    'consensus_class_ID': consensus_class_id_list,\n",
    "    'bbox': bbox_list,\n",
    "    'pielou_index': pielou_index_list\n",
    "})\n",
    "\n",
    "#Pielou Index statistics overall and per class\n",
    "print(pielou[\"pielou_index\"].mean())\n",
    "print(pielou[\"pielou_index\"].std())\n",
    "print(pielou.groupby(\"consensus_class_ID\")[\"pielou_index\"].mean())\n",
    "print(pielou.groupby(\"consensus_class_ID\")[\"pielou_index\"].std())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
